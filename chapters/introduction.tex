\chapter{Introduction}\label{section:introduction}

Before robots can be deployed in safety-critical environments, we must be able to verify that they will perform safely. Unfortunately, as robots and other cyberphysical systems become more complex, they become harder for human engineers to test, verify, and debug. For example, autonomous vehicle (AV) operators rely on a combination of low-fidelity but inexpensive testing on massively-parallel simulation and more expensive but higher fidelity testing on physical hardware. Despite large amounts of time, money, and engineering effort invested in these systems, AVs have struggled to reach the required high degree of reliability and still encounter unforeseen, life-threatening corner cases in the wild. The challenge of testing and debugging safety-critical cyberphysical systems is not particular to AVs; for instance, as the electric power grid is increasingly automated to deal with variable sources of renewable energy, or as airspace is increasingly occupied by unmanned aerial vehicles, it will be crucial to ensure that the associated prediction and control algorithms are thoroughly tested before deployment.

In cases where real-world testing is too risky or expensive, engineers must instead rely on mathematical modeling and simulation to verify that a robot will perform as intended. Unfortunately, there are issues with each of these approaches. Although mathematical models are amenable to formal proofs, robots are often too complex to reduce to a set of equations. On the other hand, although simulators can handle the full complexity of a robotic system, they are often treated as black-boxes, providing an incomplete view of a robot's performance. In order to safely deploy complex robots in the real world, we require new tools that blend the rigor of mathematical modeling with the scalability and generality of simulation.

This thesis aims to close the gap between formal methods based on mathematical models~\cite{beltaFormalMethodsControl2019,kress-gazitSynthesisRobotsGuarantees2018} and simulation-based verification techniques~\cite{zhouRoCUSRobotController2021,corsoSurveyAlgorithmsBlackBox2021,okellyScalableEndtoEndAutonomous2018} to develop tools to help engineers more easily design and verify complex robotic systems. Our key insight is that simulators, as computer programs, are not black boxes but instead contain rich mathematical structure. If we can exploit this structure using program analysis tools (e.g. automatic differentiation, tracing, etc.), we can improve the efficiency, flexibility, and rigor of simulation-based approaches.

\section{Thesis contributions}

Using this insight, we develop tools to support the design and analysis process for robots and other safety-critical cyberphysical systems in four ways:
\begin{enumerate}
    \item \textit{Design optimization:} automatically search for design parameters that achieve good performance.
    \item \textit{Safety verification:} characterize the robustness of a design and predict corner cases where it is likely to fail (by either violating a constraint or incurring a high cost).
    \item \textit{Verification-guided design:} closing the feedback loop between verification and design; e.g., by using predicted corner cases to guide future design iterations.
    \item \textit{Anomaly diagnosis:} explain the root causes of unexpected behavior observed in deployed systems.
\end{enumerate}
%
The goal of this thesis is to provide tools that will support engineers in developing increasingly complex robotic systems, enabling a more efficient design process and providing the ability to verify the safety of a design \textit{before} deployment. The following sections provide a summary of the contributions of this thesis in each of these areas before concluding with an outline of the rest of this document.

\subsection{End-to-end design optimization and robustness certification}

Practical robotic and cyberphysical systems often have diverse subsystems that do not lend themselves to easy mathematical abstraction and formal analysis. For example, it would be difficult to develop a formal mathematical model for the interactions between planning, perception, and control subsystems. Further complicating matters, these systems must operate reliably in uncertainty environments. This combination of complexity and uncertainty makes it difficult to design and verify these systems, especially when some subsystems contain machine learning models with thousands of tunable parameters and difficult-to-interpret behaviors.

To address these challenges, we develop an automated tool that enables efficient optimization and statistical robustness certification of robot designs. Our framework uses differentiable programming for end-to-end optimization of robotic systems, allowing users to flexibly model interactions between subsystems and the effect of environmental uncertainty. In addition, we develop a novel statistical framework for certifying the worst-case performance and sensitivity of optimized designs. We apply these tools to optimize the design of two robotic systems in hardware, using statistical certification to verify robustness before transferring the optimized designs to hardware without fine-tuning.

\subsection{Improving design robustness using counterexamples for formal specifications}

Although end-to-end design optimization provides a flexible means of optimizing robot behaviors, prior approaches to design optimization rely on domain randomization over a large number of environments to encourage robustness~\cite{tobinDomainRandomizationTransferring2017,dawsonCertifiableRobotDesign2022}. This approach is not only computationally expensive, but it also risks missing important corner cases if the set of training examples is too small.

In this thesis, we improve the sample efficiency of end-to-end design optimization techniques by incorporating adversarial optimization methods to search for counterexamples to formal specifications of robot behavior. We use differentiable temporal logic to specify high-level safety and performance requirements, then use adversarial optimization to search for the worst-case counterexamples falsifying these requirements. We show that feeding these adversarial counterexamples back into the design optimization process yields more robust designs while requiring far fewer samples than standard domain randomization techniques.

\subsection{Predicting and repairing diverse failure modes}

The end-to-end design and verification methods discussed so far have the advantage of using gradients from automatic differentiation to speed the search for high-performing designs and challenging failure modes, but there are a number of drawbacks to these gradient-based methods. In particular, because gradient-based methods quickly converge to local optima, they struggle to find a set of qualitatively different failure modes. Instead, nearby failure modes collapse into a single ``most severe'' mode. Moreover, high variance in the gradients found via automatic differentiation can create practical challenges for gradient-based optimization on systems with complex dynamics or visual feedback.

Building off of the adversarial optimization method discussed in the previous section, we resolve these issues via a novel reformulation of adversarial optimization as a sequential Bayesian inference problem, which allows us to develop a sampling-based algorithm for verification and design. Rather than using gradient ascent to find high-severity failure modes, we use gradient-accelerated Markov chain Monte Carlo (MCMC) algorithms to sample environmental parameters that are likely to induce failures. We find that not only do MCMC-based methods yield a more diverse and challenging set of failure modes, but they can also be used to repair the design by fine-tuning on the predicted failure modes. We demonstrate this ability to predict and repair a diverse set of failure modes on a wide range of autonomous and cyberphysical systems, including power networks, multi-agent systems, and robotic system with visual feedback. Across these examples, we find that our method yields designs with a lower failure rate than those found using existing gradient-free and gradient-based optimization techniques.

\subsection{Diagnosing anomalies from limited data}

Despite our best efforts at predicting and repairing possible failures in simulation, autonomous systems may still fail after deployment. In these cases, engineers must be able to diagnose the failure and understand its root causes before they can take corrective action. Because failures encountered in the wild are relatively rare, they can be challenging to diagnose due to the limited amount of available data. In the case of autonomous vehicles, we may have tens of thousands of miles of normal driving data for every mile of data where a failure occurred; in the case of critical infrastructure like power and transportation networks, there might be multiple years between severe failures. Although we can frame the anomaly diagnosis problem as one of Bayesian inference, existing inference methods struggle to handle this imbalanced dataset and will tend to either overfit to the limited anomaly data (which may be corrupted with noise or outliers) or else underfit the anomaly in favor of the much more abundant data from normal operations.

Existing methods require careful tuning of hyperparameters to avoid overfitting or underfitting; e.g., by adjusting the amount of regularization used when training on scarce anomaly data. In this thesis, we develop a novel hyperparameter-free method, \ouralg{}, for robust inference in this data-constrained setting. We use deep normalizing flows and take inspiration from robust regression and statistical bootstrapping methods to learn an appropriately-regularized posterior distribution for the anomaly data in a self-supervised manner.

We find that our method is able to outperform state-of-the-art regularized inference methods without requiring any hyperparameter tuning. We apply \ouralg{} to a real-world case study into the causes of the 2022 Southwest Airlines scheduling crisis, where we are able to infer how hidden changes in the distribution of aircraft within the Southwest network contributed to the disruption and allowed local disturbances to cascade into system-wide failures.

% \section{Summary of contributions}

% % TODO

\section{Outline}

This thesis is structured as follows. Chapter~\ref{section:lit_review} provides relevant background on design and verification tools for autonomous systems. Chapters~\ref{ch:rss}--\ref{ch:corl} develop a series of increasingly capable simulation-driven design and verification tools that rely on program analysis techniques to efficiently find and repair failure modes in autonomous systems. In particular, Chapter~\ref{ch:rss} introduces the end-to-end design optimization problem, demonstrates how it can be solved using differentiable simulation, and provides a statistical framework for robustness certification, Chapter~\ref{ch:iros} improves the sample efficiency of design optimization using adversarial optimization techniques, and Chapter~\ref{ch:corl} introduces a novel reformulation of this adversarial optimization approach through the lens of Bayesian inference, leading to the ability to efficiently predict and repair a diverse set of possible failure modes. While Chapters~\ref{ch:rss}--\ref{ch:corl} deal with testing an autonomous system in simulation \textit{prior} to deployment, Chapter~\ref{ch:icml} introduces a framework for diagnosing and explaining anomalies encountered \textit{after} deployment. Chapter~\ref{section:conclusion} concludes the thesis and outlines possible areas for future work.