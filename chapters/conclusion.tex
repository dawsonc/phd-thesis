\chapter{Conclusion}\label{section:conclusion}

As autonomous systems increase in scale and complexity, there will be an increased need for automated tools for designing and verifying the safety of these systems. In particular, these tools will need to be increasingly flexible in order to support the variety and complexity of practical robotic and cyberphysical systems, which often have multiple interacting subsystems and do not easily lend themselves to mathematical abstractions and formal analysis.

In this thesis, we developed a new set of simulation-driven tools that take a program analysis approach to design and verification of complex autonomous systems. By considering the end-to-end behavior of the system, as specified by a simulator, we were able to simultaneously consider interactions between different subsystems and between the system and its environment. To address the scalability challenges facing previous end-to-end verification methods, we turned to program analysis techniques like automatic differentiation to not only solve verification problems with high-dimensional search spaces but also feed the results of verification back into the design process to improve robustness. In addition, we resolve the tradeoff between local gradient-based optimization, which is efficient but can get stuck in local minima or fail due to poor-quality gradients, and black-box methods that are more robust but correspondingly more computationally expensive, through a novel Bayesian inference reformulation. Using this reformulation, we developed efficient gradient-based algorithms that can gracefully degrade to gradient-free applications.

In addition to tools that can be used to test and improve a system's performance prior to deployment, we also develop inference-based tools for diagnosing and explaining anomalies encountered after deployment. These tools also make use of program analysis methods, in particular automatic differentiation and probabilistic programming, but they are designed to work with noisy, limited real-world datasets. We show how these tools can be used to provide explanations of system failures, linking the observed behaviors of complex systems to physically interpretable root causes.

Using these tools, we were able to solve challenging design and verification problems for both robotic systems, including vision-in-the-loop control for autonomous vehicles, and real-world cyberphysical infrastructure like power grids and air transportation networks. These applications demonstrate the flexibility of our approach, where our use of general-purpose program analysis tools allows us to easily adapt to new applications, but there remain a number of interesting areas for future work.

\section{Future work}

\subsection{Integrating simulation and hardware testing}

In this thesis, we focused on simulation-driven tools for safety verification and design optimization. However, no matter how extensively a system is tested in simulation, the gap between simulation and reality means that most practical systems will also require extensive testing on hardware. These hardware tests are much more expensive than testing in simulation; as a result, engineers devote considerable effort to manually designing test campaigns that begin with simulation testing and gradually build up to more expensive hardware tests. These engineers take great care to consider which subsystems will be tested at each phase of the campaign and how (or even whether) to adjust plans for later tests based on the results of early experiments. While substantial research effort, including this thesis, has been devoted to subsystem and end-to-end testing in simulation, relatively little work has gone into understanding higher level problem of designing test campaigns that strike an optimal balance between simulation and hardware testing. Traditional approach to test campaign design take a linear, waterfall approach that proceeds step-by-step through increasingly higher-fidelity test environments, but the complexity of the system under test makes it difficult to design adaptive test campaigns that can flexibly intermix low- and high-fidelity testing to maximize the likelihood of detecting issues while minimizing the cost of experiments.

Recent advances in so-called ``real2sim'' methods that use data from hardware experiments to improve the accuracy of low-fidelity simulation environments means that it is now important to consider the interplay between simulation-based and physical testing. While historically hardware testing would be conducted after exhaustive testing in simulation, it may be more efficient to interleave simulation and physical tests so that early physical tests can be used to improve the accuracy of the simulation environment, which in turn will reduce the need for more expensive testing later on. This need points to three interesting areas of future work.


\paragraph{Uncertainty quantification of simulation-based testing}
%
Existing methods for simulation-based testing do not provide a way to estimate the risk of false positive or false negative results. Quantifying this uncertainty, which may vary even between scenarios in the same simulated environment, is important not only for calibrating users' confidence in the results of simulation testing, but also for prioritizing efforts to gather additional physical data or improve the simulation environment. For example, a user might choose to focus on improving the accuracy in regions of the state space where uncertainty is high. Future work in this area may explore extending existing methods for uncertainty quantification (e.g. model-free methods based on Gaussian process regression or conformal prediction) to this problem, where the high-dimensional search space of simulation parameters may require a novel combination of model-free and model-based methods (e.g. using differentiable and probabilistic programming).

\paragraph{Optimal design of real2sim experiments}
%
Once we are able to quantify the uncertainty of the simulation environment in different regimes, future work might extend the automatic failure prediction and repair strategies developed in this thesis to select optimally-informative hardware experiments to reduce uncertainty in the simulation environment.

\paragraph{End-to-end design of test campaigns}
Given the ability to select optimal test cases for hardware experiments, a next step for future work would be to design test campaigns that efficiently interleave simulation and hardware testing. This would requires solving an exploration/exploitation trade-off between conducting experiments intended to improve the accuracy of the simulation environment and conducting experiments designed to verify the safety of the system.

\subsection{Integrating safety verification and runtime monitoring}

In this thesis, we used predictions of likely failure modes to repair the system's design to be more robust (e.g. fine-tuning a control policy to avoid the predicted failure modes). When we repair the policy, there is a risk of that these repairs may introduce additional conservatism; for instance, we saw in Chapter~\ref{ch:corl} that repairing the policy used by an autonomous vehicle to overtake another vehicle led to less aggressive behavior that avoided the overtake manuever altogether. An alternative approach, and an interesting avenue for future work, would be to use the predicted failure modes as the basis for an online monitoring and recovery system that attempts to infer when a failure mode is imminent (e.g. by matching observations to a database of previously predicted failure modes) and then take corrective action.
